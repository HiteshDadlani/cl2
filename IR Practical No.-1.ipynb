{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f778c8-7286-46d2-a884-ab73baa5fc4b",
   "metadata": {},
   "source": [
    "# IR Practical No.-1\n",
    "                            \n",
    "\n",
    "Write a program for pre-processing of a text document such as stop word removal, stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6480a006-e22d-46df-9942-4b1412a6a353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text.txt file created successfully!\n"
     ]
    }
   ],
   "source": [
    "with open(\"text.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"Natural Language Processing (NLP) is a field of Artificial Intelligence.\n",
    "It helps computers understand, interpret, and generate human language.\n",
    "This process involves tokenization, removing stopwords, and applying stemming or lemmatization.\"\"\")\n",
    "print(\"text.txt file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ad19d8-8814-49b6-81d0-288696b300fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing (NLP) is a field of Artificial Intelligence.\n",
      "It helps computers understand, interpret, and generate human language.\n",
      "This process involves tokenization, removing stopwords, and applying stemming or lemmatization.\n"
     ]
    }
   ],
   "source": [
    "# Display the content of text.txt\n",
    "with open(\"text.txt\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ef4c9c-a0e4-478c-9c5f-c6dee6bef8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21680a6-cf55-4121-ad48-641932c08b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Hitesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hitesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hitesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "518ac26b-c076-48a1-b744-15dc736e54a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words:\n",
      " ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'Artificial', 'Intelligence', '.', 'It', 'helps', 'computers', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'This', 'process', 'involves', 'tokenization', ',', 'removing', 'stopwords', ',', 'and', 'applying', 'stemming', 'or', 'lemmatization', '.']\n",
      "\n",
      "After Removing Stop Words:\n",
      " ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'field', 'Artificial', 'Intelligence', '.', 'helps', 'computers', 'understand', ',', 'interpret', ',', 'generate', 'human', 'language', '.', 'process', 'involves', 'tokenization', ',', 'removing', 'stopwords', ',', 'applying', 'stemming', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Read the file\n",
    "with open(\"text.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_words = [w for w in words if w.lower() not in stop_words]\n",
    "\n",
    "print(\"Original Words:\\n\", words)\n",
    "print(\"\\nAfter Removing Stop Words:\\n\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77cd192-0e0e-4071-a0ea-6dfaf76292cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Stemming:\n",
      " ['natur', 'languag', 'process', '(', 'nlp', ')', 'field', 'artifici', 'intellig', '.', 'help', 'comput', 'understand', ',', 'interpret', ',', 'gener', 'human', 'languag', '.', 'process', 'involv', 'token', ',', 'remov', 'stopword', ',', 'appli', 'stem', 'lemmat', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Create stemmer object\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "\n",
    "print(\"\\nAfter Stemming:\\n\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b44f24-9907-4ead-8e2c-48aea3826f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programmers ‚Üí programm\n",
      "program ‚Üí program\n",
      "with ‚Üí with\n",
      "programming ‚Üí program\n",
      "languages ‚Üí languag\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Programmers program with programming languages\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Apply stemming\n",
    "for w in tokens:\n",
    "    print(w, \"‚Üí\", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f4a060b-a9c0-4e87-a482-1bcf75a6ea34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Sentence Tokenization:\n",
      "\n",
      "1. Natural Language Processing (NLP) is a field of Artificial Intelligence.\n",
      "2. It helps computers understand, interpret, and generate human language.\n",
      "3. This process involves tokenization, removing stopwords, and applying stemming or lemmatization.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"\\nüü¢ Sentence Tokenization:\\n\")\n",
    "for i, s in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff436ee8-4dd9-432d-ae8f-db949f433a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü£ Word Tokenization:\n",
      "\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'Artificial', 'Intelligence', '.', 'It', 'helps', 'computers', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'This', 'process', 'involves', 'tokenization', ',', 'removing', 'stopwords', ',', 'and', 'applying', 'stemming', 'or', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "# Split text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"\\nüü£ Word Tokenization:\\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ffb04-26c0-498c-a993-1ffa7eb3279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's great! I can certainly break down this code for you and anticipate the key questions your teacher might ask during a practical exam on Information Retrieval (IR) and Natural Language Processing (NLP) pre-processing.\n",
    "\n",
    "# ## üéì IR Practical No. 1: Text Pre-processing (NLP Foundation)\n",
    "\n",
    "# This practical demonstrates the core steps required to clean and normalize raw text data before it can be effectively used in NLP tasks like searching (Information Retrieval), classification, or analysis.\n",
    "\n",
    "# ### 1. ‚öôÔ∏è Setup and Dependencies (Cells 3 & 4)\n",
    "\n",
    "# | Component | Code | Purpose | Potential Questions |\n",
    "# | :--- | :--- | :--- | :--- |\n",
    "# | **Libraries** | `import nltk`, `from nltk.corpus import stopwords`, `from nltk.tokenize import word_tokenize`, `from nltk.stem import PorterStemmer` | Imports the necessary tools from NLTK. | **What is NLTK?** (Answer: Natural Language Toolkit, a leading Python library for working with human language data.) |\n",
    "# | **Downloading** | `nltk.download('punkt')`, `nltk.download('stopwords')` | Downloads essential data packages: `punkt` (for tokenization) and `stopwords` (the list of common words). | **Why do we need to download packages like 'punkt' and 'stopwords'?** (Answer: These are data files/models that NLTK uses. They are large and kept separate from the main library installation.) |\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 2. ‚úÇÔ∏è Tokenization (Cells 5, 7, 8 & 9)\n",
    "\n",
    "# Tokenization is the process of breaking text into smaller, meaningful units called \"tokens.\"\n",
    "\n",
    "# | Technique | Code Example | Output | Concept | Potential Questions |\n",
    "# | :--- | :--- | :--- | :--- | :--- |\n",
    "# | **Word Tokenization** | `words = word_tokenize(text)` | `['Natural', 'Language', 'Processing', '(', 'NLP', ')', ...]` \n",
    "#| Breaks the text into individual words, numbers, and punctuation marks. | \n",
    "#**What is a Token?** \n",
    "#(Answer: A token is an instance of a sequence of characters in some document that are grouped together as a useful semantic unit.) |\n",
    "# | **Sentence Tokenization** | `sentences = sent_tokenize(text)` | Each sentence is a separate element. | \n",
    "#Breaks the text into individual sentences. | \n",
    "#**What is the difference between Word and Sentence Tokenization?** \n",
    "#(Answer: Word tokenization gives you individual terms; sentence tokenization gives you a list of sentences, \n",
    "#usually using punctuation like '.', '?' or '!' as delimiters.) |\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 3. üõë Stop Word Removal (Cell 5)\n",
    "\n",
    "# Stop words are extremely common words (like 'the', 'a', 'is', 'it') that generally do not carry significant meaning for retrieval or classification tasks. Removing them reduces the size of the vocabulary and improves efficiency.\n",
    "\n",
    "# | Step | Code | Purpose |\n",
    "# | :--- | :--- | :--- |\n",
    "# | **Define Set** | `stop_words = set(stopwords.words('english'))` | Loads a standard list of common English stop words. Using a `set` makes lookup very fast. |\n",
    "# | **Filtering** | `filtered_words = [w for w in words if w.lower() not in stop_words]` | Iterates through the tokens (`words`). If a word (converted to lowercase) is **not** in the `stop_words` set, it is kept in the `filtered_words` list. |\n",
    "\n",
    "# **Potential Questions:**\n",
    "\n",
    "# * **Why do we remove stop words?** \n",
    "#(Answer: To reduce noise, decrease the size of the index/vocabulary, and improve search efficiency/relevance by focusing on meaningful terms.)\n",
    "# * **Why convert words to lowercase before checking against `stop_words`?** \n",
    "#(Answer: To ensure consistency. 'Is' at the start of a sentence and 'is' elsewhere should both be recognized as the stop word 'is'.)\n",
    "# * **In which scenarios might you NOT remove stop words?**\n",
    "#(Answer: When the order and grammatical structure are important, such as in Named Entity Recognition (NER) or Machine Translation.)\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 4. üî™ Stemming (Cells 6 & 7)\n",
    "\n",
    "# Stemming is a heuristic process that chops off the ends of words in the hope of achieving the root form, called a \"stem.\"\n",
    "\n",
    "# | Step | Code | Purpose |\n",
    "# | :--- | :--- | :--- |\n",
    "# | **Stemmer Object** | `ps = PorterStemmer()` | Creates an instance of the Porter Stemmer algorithm (the most common and simple stemmer). |\n",
    "# | **Applying Stemming** | `stemmed_words = [ps.stem(w) for w in filtered_words]` | Applies the stemming algorithm to each word in the list. |\n",
    "\n",
    "# **Example of Stemming:**\n",
    "\n",
    "# | Original Word | Stemmed Word |\n",
    "# | :--- | :--- |\n",
    "# | Programmers | programm |\n",
    "# | programming | program |\n",
    "# | languages | languag |\n",
    "\n",
    "# **Potential Questions:**\n",
    "\n",
    "# * **What is the purpose of Stemming?** (Answer: It is a normalization technique to reduce inflectional forms and sometimes derivational\n",
    "#forms of a word to a common base form. This allows a search for \"programming\" to also match documents containing \"programmers\" or \"program,\"\n",
    "#increasing Recall.)\n",
    "# * **What is the difference between Stemming and Lemmatization?** (This is the most common follow-up question!)\n",
    "#     * **Stemming:** A fast, crude heuristic process that chops off suffixes. The resulting \"stem\" is often **not** a valid word\n",
    "#(e.g., 'Intelligent' ‚Üí 'intellig').\n",
    "#     * **Lemmatization:** A more sophisticated process that uses a vocabulary and morphological analysis (part-of-speech tagging)\n",
    "#to return the base/dictionary form of a word (the **lemma**), which is always a valid word (e.g., 'better' ‚Üí 'good').\n",
    "# * **What is the Porter Stemmer?** (Answer: The most widely used stemming algorithm for English, developed by Martin Porter in 1979.)\n",
    "\n",
    "# This detailed breakdown covers the code, the underlying NLP concepts, and the most likely questions you'll face. Good luck with your exam!\n",
    "\n",
    "# ---\n",
    "\n",
    "# **Would you like me to focus on one of these concepts (e.g., Stemming vs. Lemmatization) in more detail?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
